{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import & constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "import numpy as np\n",
    "\n",
    "from thaig2p import decode, validate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from IPython.display import Image, display_png\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Thai Phonology\n",
    "SHORT_VOWELS = \"aivueyoxz\"\n",
    "LONG_VOWELS =  \"AIVUEYOXZ\"\n",
    "DIPHTHONGS = \"JWR\"\n",
    "VOWELS = SHORT_VOWELS + LONG_VOWELS + DIPHTHONGS\n",
    "ONSETS = [\"br\",\"bl\",\"pr\",\"pl\",\"Pr\",\"Pl\",\"fr\",\"fl\",\"dr\",\"tr\",\"Tr\",\"kr\",\"kl\",\"kw\",\"Kr\",\"Kl\",\"Kw\"] + \\\n",
    "    [\"b\",\"p\",\"P\",\"m\",\"f\",\"d\",\"t\",\"T\",\"n\",\"s\",\"r\",\"l\",\"c\",\"C\",\"k\",\"K\",\"N\",\"w\",\"j\",\"h\",\"?\"]\n",
    "CODAS = [\"p\",\"m\",\"f\",\"t\",\"d\",\"n\",\"s\",\"l\",\"c\",\"k\",\"N\",\"w\",\"j\",\"?\",\"-\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>^กก$</td>\n",
       "      <td>kok2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>^กกกอด$</td>\n",
       "      <td>kok2 kXt2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>^กกช้าง$</td>\n",
       "      <td>kok2 CAN4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>^กกธูป$</td>\n",
       "      <td>kok2 TUp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>^กกหู$</td>\n",
       "      <td>kok2 hU-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34544</th>\n",
       "      <td>^สิทธิพงษ์$</td>\n",
       "      <td>sit2 Ti-4 PoN1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34545</th>\n",
       "      <td>^ธีรวีร์$</td>\n",
       "      <td>TI-1 ra-1 wI-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34546</th>\n",
       "      <td>^นิภากร$</td>\n",
       "      <td>ni-4 PA-1 kXn1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34547</th>\n",
       "      <td>^จุฑาธิป$</td>\n",
       "      <td>cu-2 TA-1 Tip4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34548</th>\n",
       "      <td>^นาราภัทร$</td>\n",
       "      <td>nA-1 rA-1 Pat4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34549 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 g               p\n",
       "0             ^กก$            kok2\n",
       "1          ^กกกอด$       kok2 kXt2\n",
       "2         ^กกช้าง$       kok2 CAN4\n",
       "3          ^กกธูป$       kok2 TUp3\n",
       "4           ^กกหู$       kok2 hU-5\n",
       "...            ...             ...\n",
       "34544  ^สิทธิพงษ์$  sit2 Ti-4 PoN1\n",
       "34545    ^ธีรวีร์$  TI-1 ra-1 wI-1\n",
       "34546     ^นิภากร$  ni-4 PA-1 kXn1\n",
       "34547    ^จุฑาธิป$  cu-2 TA-1 Tip4\n",
       "34548   ^นาราภัทร$  nA-1 rA-1 Pat4\n",
       "\n",
       "[34549 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load Thai Name data\n",
    "train_x = open('data/train_x.txt').read().splitlines()\n",
    "train_y = open('data/train_y.txt').read().splitlines()\n",
    "train_y_ipa = open('data/train_y_ipa.txt').read().splitlines()\n",
    "test_x = open('data/test_x.txt').read().splitlines()\n",
    "test_y = open('data/test_y.txt').read().splitlines()\n",
    "test_y_ipa = open('data/test_y_ipa.txt').read().splitlines()\n",
    "\n",
    "### dict data\n",
    "data = pd.read_csv('data/thai2phone.csv', names=['g','p'])\n",
    "data = data[~data.g.str.contains('\\.')] # exclude abbreviation\n",
    "data = data.dropna()\n",
    "\n",
    "### merge Name & dict\n",
    "data = pd.concat([data, pd.DataFrame({'g':train_x, 'p':train_y_ipa})]).reset_index(drop=True)\n",
    "\n",
    "### add <EOS>= ^  and <SOS> = $ for train/test X\n",
    "test_x = ['^'+x+'$' for x in test_x] \n",
    "data['g'] = data.g.apply(lambda x: '^'+x+'$')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syl_split(syl):\n",
    "    tone = syl[-1] # prA-1 -> 1\n",
    "    coda = syl[-2] # prA-1 -> -\n",
    "    vowel = syl[-3] # prA-1 -> A\n",
    "    onset = syl[:-3] # # prA-1 -> pr\n",
    "    return onset, vowel, coda, tone\n",
    "\n",
    "def make_output(phon, n=0): # n: onset, vowel, coda, tone\n",
    "    syls = phon.split()\n",
    "    splitted = [syl_split(syl)[n] for syl in syls]\n",
    "    if n==0:\n",
    "        return [onset2id[x] for x in splitted]\n",
    "    elif n==1:\n",
    "        return [vowel2id[x] for x in splitted]\n",
    "    elif n==2:\n",
    "        return [coda2id[x] for x in splitted]\n",
    "    elif n==3:\n",
    "        return [tone2id[x] for x in splitted]\n",
    "\n",
    "thai2id = {'<PAD>':0, '^':1, '$':2, ' ':3}\n",
    "for i in range(3585, 3674): # ก=3585->4, ๙=3673\n",
    "    thai2id[chr(i)] = i - 3581\n",
    "id2thai = {v:k for k,v in thai2id.items()}\n",
    "\n",
    "onset2id = {'<PAD>':0,'<SOS>':1,'<EOS>':2}\n",
    "for c in ONSETS:\n",
    "    onset2id[c] = max(onset2id.values()) + 1\n",
    "id2onset = {v:k for k,v in onset2id.items()}\n",
    "\n",
    "coda2id = {'<PAD>':0,'<SOS>':1,'<EOS>':2}\n",
    "for c in CODAS:\n",
    "    coda2id[c] = max(coda2id.values()) + 1\n",
    "id2coda = {v:k for k,v in coda2id.items()}\n",
    "\n",
    "vowel2id = {'<PAD>':0,'<SOS>':1,'<EOS>':2}\n",
    "for c in VOWELS:\n",
    "    vowel2id[c] = max(vowel2id.values()) + 1\n",
    "id2vowel = {v:k for k,v in vowel2id.items()}\n",
    "\n",
    "tone2id = {'<PAD>':0,'<SOS>':1,'<EOS>':2,'1':3,'2':4,'3':5,'4':6,'5':7}\n",
    "id2tone = {v:k for k,v in tone2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# max length of input characters\n",
    "print(max(map(len, data.g)))\n",
    "\n",
    "# max length of output syllables\n",
    "print(max([len(x.split(\" \")) for x in data.p]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## indexing, padding, encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input: (34549, 55) (700, 55)\n",
      "Decoder Onset: (34549, 22)\n",
      "Decoder Vowel: (34549, 22)\n",
      "Decoder Coda : (34549, 22)\n",
      "Decoder Tone : (34549, 22)\n",
      "Decoder BIO  : (34549, 22)\n",
      "[ 1 34 30 28 32 21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[ 1  3  3 13  3  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[ 1 17 17 17  4  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[1 4 6 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 2 2 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "output_maxlen = 22\n",
    "\n",
    "### encoder input - Thai characters\n",
    "x_enc = [[thai2id[c] for c in word] for word in data.g]\n",
    "x_enc = pad_sequences(x_enc, padding='post', maxlen=55, value=0)\n",
    "\n",
    "x_enc_test = [[thai2id[c] for c in word] for word in test_x]\n",
    "x_enc_test = pad_sequences(x_enc_test, padding='post', maxlen=55, value=0)\n",
    "\n",
    "\n",
    "### decoder input - each syllable\n",
    "x_dec0 = [[1]+make_output(phon, n=0) for phon in data.p] # onset\n",
    "x_dec1 = [[1]+make_output(phon, n=1) for phon in data.p] # vowel\n",
    "x_dec2 = [[1]+make_output(phon, n=2) for phon in data.p] # coda\n",
    "x_dec3 = [[1]+make_output(phon, n=3) for phon in data.p] # tone\n",
    "x_dec4 = [[1]+[2]*(len(seq)-2)+[3] for seq in x_dec0] # BIEO tagging <SOS>=1, <IN>=2, <EOS>=3, <PAD>=0\n",
    "\n",
    "\n",
    "### padding ###\n",
    "x_dec0 = pad_sequences(x_dec0, padding='post', maxlen=output_maxlen, value=0)\n",
    "x_dec1 = pad_sequences(x_dec1, padding='post', maxlen=output_maxlen, value=0)\n",
    "x_dec2 = pad_sequences(x_dec2, padding='post', maxlen=output_maxlen, value=0)\n",
    "x_dec3 = pad_sequences(x_dec3, padding='post', maxlen=output_maxlen, value=0)\n",
    "x_dec4 = pad_sequences(x_dec4, padding='post', maxlen=output_maxlen, value=0)\n",
    "\n",
    "\n",
    "### check ###\n",
    "print('Encoder Input:', x_enc.shape, x_enc_test.shape)\n",
    "print('Decoder Onset:', x_dec0.shape) # onset\n",
    "print('Decoder Vowel:', x_dec1.shape) # vowel\n",
    "print('Decoder Coda :', x_dec2.shape) # coda\n",
    "print('Decoder Tone :', x_dec3.shape) # tone\n",
    "print('Decoder BIO  :', x_dec4.shape) # BIEO tagging\n",
    "\n",
    "i = 100\n",
    "\n",
    "print(x_dec0[i]) # begin with 1 = <SOS>\n",
    "print(x_dec1[i])\n",
    "print(x_dec2[i])\n",
    "print(x_dec3[i])\n",
    "print(x_dec4[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output [Onset]: (34549, 22, 41)\n",
      "Output [Vowel]: (34549, 22, 24)\n",
      "Output [Coda] : (34549, 22, 18)\n",
      "Output [Tone] : (34549, 22, 8)\n",
      "Output [BIEO] : (34549, 22, 4)\n"
     ]
    }
   ],
   "source": [
    "### one-hot vector for decoder output ###\n",
    "y0 = to_categorical(x_dec0, num_classes=len(onset2id)) # onset\n",
    "y1 = to_categorical(x_dec1, num_classes=len(vowel2id)) # vowel\n",
    "y2 = to_categorical(x_dec2, num_classes=len(coda2id)) # coda\n",
    "y3 = to_categorical(x_dec3, num_classes=8) # tone = 3-7, <PAD> = 0, <SOS> = 1, <EOS> = 2\n",
    "y4 = to_categorical(x_dec4, num_classes=4) # 1, 2, 3, 0\n",
    "\n",
    "print('Output [Onset]:', y0.shape)\n",
    "print('Output [Vowel]:', y1.shape)\n",
    "print('Output [Coda] :', y2.shape)\n",
    "print('Output [Tone] :', y3.shape)\n",
    "print('Output [BIEO] :', y4.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "input_vocab = len(thai2id)\n",
    "input_len = x_enc.shape[1]\n",
    "output_len = output_maxlen-1 # delete <SOS> for decoder target\n",
    "enc_emb_dim = 300\n",
    "dec_emb_dim = 200\n",
    "dec_bio_dim = 40\n",
    "enc_lstm_dim = (dec_emb_dim*4 + dec_bio_dim) // 2\n",
    "dec_lstm_dim = dec_emb_dim*4 + dec_bio_dim\n",
    "dec_hidden_dim = 100\n",
    "lstm_dropout = 0.2\n",
    "\n",
    "### ENCODER ###\n",
    "input_enc= layers.Input(batch_size=None, shape=(input_len,), name='Enc_Input')\n",
    "emb_enc = layers.Embedding(input_dim = input_vocab, # num of vocab = num of ID dict\n",
    "                                    input_length = input_len,\n",
    "                                    output_dim = enc_emb_dim,\n",
    "                                    mask_zero = True,\n",
    "                                    name = 'Enc_Embedding')\n",
    "lstm_enc = layers.Bidirectional(layers.LSTM(enc_lstm_dim,\n",
    "                                        return_sequences=True, # use only final output when w/o attention -> return (batch, lstm_dim)\n",
    "                                        return_state=True,\n",
    "                                        recurrent_dropout=lstm_dropout,\n",
    "                                        name='Enc_LSTM'), name='BiDir') # return: output, h1, c1, h2, c2\n",
    "concat_h = layers.Concatenate(name='Concat_h') # concatnate bidirectional output of state h \n",
    "concat_c = layers.Concatenate(name='Concat_c') # concatnate bidirectional output of state c\n",
    "\n",
    "### DECODER ###\n",
    "class Attention:\n",
    "    def __init__(self, unit=256, emb_dim=256):\n",
    "        self.dot = layers.Dot(axes=[2,2], name='Attn_Dot') # (batch, dec_seq_length, \"lstm_dim\") * (batch, enc_seq_length, \"lstm_dim\") -> weight (batch, dec_seq_length, enc_seq_length) \n",
    "        #self.sqrt = layers.Lambda(lambda x: x/emb_dim**0.5, name='Sqrt') # scaling by square root : */√dim\n",
    "        self.softmax = layers.Activation(activation='softmax', name='Attn_Softmax')\n",
    "        self.context = layers.Dot(axes=[2,1], name='Attn_Context') # weight (batch, dec_seq_length, \"enc_seq_length\") * (batch, \"enc_seq_length\", dim) -> (batch, dec_seq_length, dim)\n",
    "        self.concat = layers.Concatenate(name='Attn_Concat') # concate context and decoder output\n",
    "        self.hidden = layers.Dense(unit, activation='tanh', name='Attn_hidden')\n",
    "\n",
    "    def __call__(self, enc_output, dec_output):\n",
    "        attention_dot = self.dot([dec_output, enc_output])\n",
    "        #attention_weight = self.softmax(self.sqrt(attention_dot))\n",
    "        attention_weight = self.softmax(attention_dot)\n",
    "        context_vector = self.context([attention_weight, enc_output])\n",
    "        concat_vector = self.concat([context_vector, dec_output])\n",
    "        return self.hidden(concat_vector)\n",
    "\n",
    "class Decoder:\n",
    "    def __init__(self):\n",
    "        self.input0 = layers.Input(batch_size=None, shape=(output_len,), name='Onset_Input') # onset\n",
    "        self.input1 = layers.Input(batch_size=None, shape=(output_len,), name='Vowel_Input') # vowel\n",
    "        self.input2 = layers.Input(batch_size=None, shape=(output_len,), name='Coda_Input') # coda\n",
    "        self.input3 = layers.Input(batch_size=None, shape=(output_len,), name='Tone_Input') # tone\n",
    "        self.input4 = layers.Input(batch_size=None, shape=(output_len,), name='BIO_Input') # syllables left\n",
    "        self.input_h = layers.Input(batch_size=None, shape=(enc_lstm_dim*2,), name='Dec_Input_h') # only for predict\n",
    "        self.input_c = layers.Input(batch_size=None, shape=(enc_lstm_dim*2,), name='Dec_Input_c') # only for predict\n",
    "        self.emb_dec0 = layers.Embedding(input_dim=len(onset2id), input_length=output_len, output_dim=dec_emb_dim, mask_zero=True, name = 'Dec_Emb0')\n",
    "        self.emb_dec1 = layers.Embedding(input_dim=len(vowel2id), input_length=output_len, output_dim=dec_emb_dim, mask_zero=True, name = 'Dec_Emb1')\n",
    "        self.emb_dec2 = layers.Embedding(input_dim=len(coda2id), input_length=output_len, output_dim=dec_emb_dim, mask_zero=True, name = 'Dec_Emb2')\n",
    "        self.emb_dec3 = layers.Embedding(input_dim=len(tone2id), input_length=output_len, output_dim=dec_emb_dim, mask_zero=True, name = 'Dec_Emb3')\n",
    "        self.emb_dec4 = layers.Embedding(input_dim=output_maxlen+1, input_length=output_len, output_dim=dec_bio_dim, mask_zero=True, name = 'Dec_Emb4') # BIEO tagging\n",
    "        self.concat_emb = layers.Concatenate(name='Concate_emb') # cancatenate all embeddings\n",
    "        self.lstm_dec = layers.LSTM(dec_lstm_dim, # twice as lstm dim of encoder\n",
    "                                return_sequences=True,\n",
    "                                return_state=True,\n",
    "                                recurrent_dropout=lstm_dropout,\n",
    "                                name='Dec_LSTM')\n",
    "        self.attention = Attention(unit=dec_lstm_dim, emb_dim=dec_lstm_dim)\n",
    "        self.dense = layers.Dense(dec_hidden_dim, name='Dec_Hidden')\n",
    "        self.dense0 = layers.Dense(len(onset2id), activation='softmax', name='Onset')\n",
    "        self.dense1 = layers.Dense(len(vowel2id), activation='softmax', name='Vowel')\n",
    "        self.dense2 = layers.Dense(len(coda2id), activation='softmax', name='Coda')\n",
    "        self.dense3 = layers.Dense(len(tone2id), activation='softmax', name='Tone')\n",
    "        self.dense4 = layers.Dense(4, activation='softmax', name='BIEO')\n",
    "\n",
    "    def __call__(self, encoder_output, input_h, input_c):\n",
    "        x0 = self.emb_dec0(self.input0)\n",
    "        x1 = self.emb_dec1(self.input1)\n",
    "        x2 = self.emb_dec2(self.input2)\n",
    "        x3 = self.emb_dec3(self.input3)\n",
    "        x4 = self.emb_dec4(self.input4)\n",
    "        emb_dec = self.concat_emb([x0,x1,x2,x3,x4])\n",
    "        mask = self.emb_dec0.compute_mask(self.input0) # create mask for LSTM (because mask of Embeddinng is overwriten after concat)\n",
    "        dec_output, h, c = self.lstm_dec(emb_dec, initial_state=[input_h, input_c], mask=mask) # for train, input_h and input_c are output of encoder\n",
    "        x = self.attention(enc_output, dec_output)\n",
    "        x = self.dense(x)\n",
    "        output0 = self.dense0(x)\n",
    "        output1 = self.dense1(x)\n",
    "        output2 = self.dense2(x)\n",
    "        output3 = self.dense3(x)\n",
    "        output4 = self.dense4(x)\n",
    "        return output0, output1, output2, output3, output4, h, c\n",
    "\n",
    "\n",
    "## Encoder call\n",
    "enc_output, h1, c1, h2, c2 = lstm_enc(emb_enc(input_enc))\n",
    "enc_state_h = concat_h([h1, h2])\n",
    "enc_state_c = concat_c([c1, c2])\n",
    "\n",
    "## Decoder call\n",
    "decoder = Decoder()\n",
    "output0, output1, output2, output3, output4, h, c = decoder(enc_output, enc_state_h, enc_state_h)\n",
    "output0_pred, output1_pred, output2_pred, output3_pred, output4_pred, h_pred, c_pred = decoder(enc_output, decoder.input_h, decoder.input_c)\n",
    "\n",
    "\n",
    "### Build Models\n",
    "## for train\n",
    "model = keras.models.Model(inputs=[\n",
    "    input_enc,\n",
    "    decoder.input0,\n",
    "    decoder.input1,\n",
    "    decoder.input2,\n",
    "    decoder.input3,\n",
    "    decoder.input4], outputs=[\n",
    "    output0,\n",
    "    output1,\n",
    "    output2,\n",
    "    output3,\n",
    "    output4])\n",
    "## for predict\n",
    "model_pred_enc = keras.models.Model(input_enc, [enc_output, enc_state_h, enc_state_c])\n",
    "\n",
    "model_pred_dec = keras.models.Model(inputs=[\n",
    "    enc_output,\n",
    "    decoder.input0,\n",
    "    decoder.input1,\n",
    "    decoder.input2,\n",
    "    decoder.input3,\n",
    "    decoder.input4,\n",
    "    decoder.input_h,\n",
    "    decoder.input_c], outputs=[\n",
    "    output0_pred,\n",
    "    output1_pred,\n",
    "    output2_pred,\n",
    "    output3_pred,\n",
    "    output4_pred,\n",
    "    h_pred,\n",
    "    c_pred])\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics='accuracy')\n",
    "\n",
    "plot_model(model, show_shapes=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=[\n",
    "    x_enc,\n",
    "    x_dec0[:,:-1],\n",
    "    x_dec1[:,:-1],\n",
    "    x_dec2[:,:-1],\n",
    "    x_dec3[:,:-1],\n",
    "    x_dec4[:,:-1]], y=[\n",
    "    y0[:,1:,:],\n",
    "    y1[:,1:,:],\n",
    "    y2[:,1:,:],\n",
    "    y3[:,1:,:],\n",
    "    y4[:,1:]\n",
    "    ],\n",
    "epochs=5,\n",
    "validation_split=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one(input_seq): # input must be 2D array\n",
    "    enc_output, h, c = model_pred_enc.predict(input_seq, verbose=0) # verbose=0: not print\n",
    "    target_seq0 = np.zeros((1, output_len)) # generate target seq of length 1 e.g. [[1,0,0,..]]\n",
    "    target_seq1 = np.zeros((1, output_len)) # generate target seq of length 1 e.g. [[1,0,0,..]]\n",
    "    target_seq2 = np.zeros((1, output_len)) # generate target seq of length 1 e.g. [[1,0,0,..]]\n",
    "    target_seq3 = np.zeros((1, output_len)) # generate target seq of length 1 e.g. [[1,0,0,..]]\n",
    "    target_seq4 = np.zeros((1, output_len)) # generate target seq of length 1 e.g. [[1,0,0,..]]\n",
    "    target_seq0[0, 0] = 1 # <sos> ID=1\n",
    "    target_seq1[0, 0] = 1 # <sos> ID=1\n",
    "    target_seq2[0, 0] = 1 # <sos> ID=1\n",
    "    target_seq3[0, 0] = 1 # <sos> ID=1\n",
    "    target_seq4[0, 0] = 1 # <sos> ID=1\n",
    "    decoded = ''\n",
    "    for _ in range(output_len):\n",
    "        onset, vowel, coda, tone, bieo, h, c = model_pred_dec.predict([enc_output, target_seq0, target_seq1, target_seq2, target_seq3, target_seq4, h, c], verbose=0) # not print\n",
    "        target_seq0[0,0] = np.argmax(onset[0][0])\n",
    "        target_seq1[0,0] = np.argmax(vowel[0][0])\n",
    "        target_seq2[0,0] = np.argmax(coda[0][0])\n",
    "        target_seq3[0,0] = np.argmax(tone[0][0])\n",
    "        target_seq4[0,0] = np.argmax(bieo[0][0])\n",
    "        decoded += (id2onset[np.argmax(onset[0][0])] + id2vowel[np.argmax(vowel[0][0])] + id2coda[np.argmax(coda[0][0])] + id2tone[np.argmax(tone[0][0])] + ' ')\n",
    "        if np.argmax(bieo[0][0]) == 3: # <EOS>\n",
    "            break\n",
    "    return decoded.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = [predict_one(x_enc_test[i:i+1]) for i in range(len(x_enc_test)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pra-1 PA-1 PXn1\n",
      "praphaphan\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "\n",
    "print(predict_one(x_enc_test[i:i+1]))\n",
    "print(test_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts_ipa = [decode(x, 'rtgs', False) for x in predicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7028571428571428\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thai</th>\n",
       "      <th>pred</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>^นันทิยา$</td>\n",
       "      <td>nanthiya</td>\n",
       "      <td>nanthiya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>^มนต์ชัย$</td>\n",
       "      <td>monchai</td>\n",
       "      <td>monchai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>^ประภาพรรณ$</td>\n",
       "      <td>praphaphan</td>\n",
       "      <td>praphaphan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>^เกณิกา$</td>\n",
       "      <td>kenika</td>\n",
       "      <td>kenika</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>^สมพล$</td>\n",
       "      <td>somphon</td>\n",
       "      <td>somphon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>^ปิยะนุช$</td>\n",
       "      <td>pipinut</td>\n",
       "      <td>piyanut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>^วิญญู$</td>\n",
       "      <td>winyu</td>\n",
       "      <td>winyu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>^พิมพ์พิชชา$</td>\n",
       "      <td>phimphitcha</td>\n",
       "      <td>phimphitcha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>^อารีนี$</td>\n",
       "      <td>arini</td>\n",
       "      <td>arini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>^จิรชัย$</td>\n",
       "      <td>chirachai</td>\n",
       "      <td>chirachai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Thai         pred       answer\n",
       "0       ^นันทิยา$     nanthiya     nanthiya\n",
       "1       ^มนต์ชัย$      monchai      monchai\n",
       "2     ^ประภาพรรณ$   praphaphan   praphaphan\n",
       "3        ^เกณิกา$       kenika       kenika\n",
       "4          ^สมพล$      somphon      somphon\n",
       "..            ...          ...          ...\n",
       "695     ^ปิยะนุช$      pipinut      piyanut\n",
       "696       ^วิญญู$        winyu        winyu\n",
       "697  ^พิมพ์พิชชา$  phimphitcha  phimphitcha\n",
       "698      ^อารีนี$        arini        arini\n",
       "699      ^จิรชัย$    chirachai    chirachai\n",
       "\n",
       "[700 rows x 3 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.DataFrame({'Thai':test_x, 'pred':predicts_ipa,'answer':test_y})\n",
    "print(accuracy_score(pred_df['answer'], pred_df['pred']))\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('pred_ipa.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
