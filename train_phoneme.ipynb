{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from IPython.display import Image, display_png\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHORT_VOWELS = \"aivueyoxz\"\n",
    "LONG_VOWELS =  \"AIVUEYOXZ\"\n",
    "DIPHTHONGS = \"JWR\"\n",
    "VOWELS = SHORT_VOWELS + LONG_VOWELS + DIPHTHONGS\n",
    "ONSETS = [\"br\",\"bl\",\"pr\",\"pl\",\"Pr\",\"Pl\",\"fr\",\"fl\",\"dr\",\"tr\",\"Tr\",\"kr\",\"kl\",\"kw\",\"Kr\",\"Kl\",\"Kw\"] + \\\n",
    "    [\"b\",\"p\",\"P\",\"m\",\"f\",\"d\",\"t\",\"T\",\"n\",\"s\",\"r\",\"l\",\"c\",\"C\",\"k\",\"K\",\"N\",\"w\",\"j\",\"h\",\"?\"]\n",
    "CODAS = [\"p\",\"m\",\"f\",\"t\",\"d\",\"n\",\"s\",\"l\",\"c\",\"k\",\"N\",\"w\",\"j\",\"?\",\"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>^กก$</td>\n",
       "      <td>kok2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>^กกกอด$</td>\n",
       "      <td>kok2 kXt2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>^กกช้าง$</td>\n",
       "      <td>kok2 CAN4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>^กกธูป$</td>\n",
       "      <td>kok2 TUp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>^กกหู$</td>\n",
       "      <td>kok2 hU-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31478</th>\n",
       "      <td>^ไฮโล$</td>\n",
       "      <td>haj1 lo-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31479</th>\n",
       "      <td>^ไฮไดรด์$</td>\n",
       "      <td>haj1 daj1 ra-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31480</th>\n",
       "      <td>^ไฮไฟ$</td>\n",
       "      <td>haj1 faj1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31481</th>\n",
       "      <td>^ไฮไลต์$</td>\n",
       "      <td>haj1 laj1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31482</th>\n",
       "      <td>^ไฮไลท์$</td>\n",
       "      <td>haj1 laj1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31483 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               g               p\n",
       "0           ^กก$            kok2\n",
       "1        ^กกกอด$       kok2 kXt2\n",
       "2       ^กกช้าง$       kok2 CAN4\n",
       "3        ^กกธูป$       kok2 TUp3\n",
       "4         ^กกหู$       kok2 hU-5\n",
       "...          ...             ...\n",
       "31478     ^ไฮโล$       haj1 lo-1\n",
       "31479  ^ไฮไดรด์$  haj1 daj1 ra-4\n",
       "31480     ^ไฮไฟ$       haj1 faj1\n",
       "31481   ^ไฮไลต์$       haj1 laj1\n",
       "31482   ^ไฮไลท์$       haj1 laj1\n",
       "\n",
       "[31483 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/thai2phone.csv', names=['g','p'])\n",
    "data = data[~data.g.str.contains('\\.')] # exclude abbreviation\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "data['g'] = data.g.apply(lambda x: '^'+x+'$')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syl_split(syl):\n",
    "    tone = syl[-1] # prA-1 -> 1\n",
    "    coda = syl[-2] # prA-1 -> -\n",
    "    vowel = syl[-3] # prA-1 -> A\n",
    "    onset = syl[:-3] # # prA-1 -> pr\n",
    "    return onset, vowel, coda, tone\n",
    "\n",
    "def make_output(phon, n=0): # n: onset, vowel, coda, tone\n",
    "    syls = phon.split()\n",
    "    splitted = [syl_split(syl)[n] for syl in syls]\n",
    "    if n==0:\n",
    "        return [onset2id[x] for x in splitted]\n",
    "    elif n==1:\n",
    "        return [vowel2id[x] for x in splitted]\n",
    "    elif n==2:\n",
    "        return [coda2id[x] for x in splitted]\n",
    "    elif n==3:\n",
    "        return [tone2id[x] for x in splitted]\n",
    "\n",
    "thai2id = {'<PAD>':0, '^':1, '$':2, ' ':3}\n",
    "for i in range(3585, 3674): # ก=3585->4, ๙=3673\n",
    "    thai2id[chr(i)] = i - 3581\n",
    "id2thai = {v:k for k,v in thai2id.items()}\n",
    "\n",
    "onset2id = {'<PAD>':0,'<SOS>':1}\n",
    "for c in ONSETS:\n",
    "    onset2id[c] = max(onset2id.values()) + 1\n",
    "id2onset = {v:k for k,v in onset2id.items()}\n",
    "\n",
    "coda2id = {'<PAD>':0,'<SOS>':1}\n",
    "for c in CODAS:\n",
    "    coda2id[c] = max(coda2id.values()) + 1\n",
    "id2coda = {v:k for k,v in coda2id.items()}\n",
    "\n",
    "vowel2id = {'<PAD>':0,'<SOS>':1}\n",
    "for c in VOWELS:\n",
    "    vowel2id[c] = max(vowel2id.values()) + 1\n",
    "id2vowel = {v:k for k,v in vowel2id.items()}\n",
    "\n",
    "tone2id = {'<PAD>':0,'<SOS>':1,'1':2,'2':3,'3':4,'4':5,'5':6}\n",
    "id2tone = {v:k for k,v in tone2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ka-2 ra-4 nI-1 cam1 pen1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3, 5, 2, 2, 2]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.p[100])\n",
    "make_output(data.p[100], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# max len of input character length\n",
    "print(max(map(len, data.g)))\n",
    "\n",
    "# max len of output syllable length\n",
    "print(max([len(x.split(\" \")) for x in data.p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## validation of data ##\n",
    "for phon in data.p:\n",
    "    try:\n",
    "        make_output(phon, n=3)\n",
    "    except:\n",
    "        print(phon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = [[thai2id[c] for c in word] for word in data.g]\n",
    "input_seq = pad_sequences(input_seq, padding='post', maxlen=55, value=0)\n",
    "\n",
    "output0 = [[1]+make_output(phon, n=0) for phon in data.p] # onset\n",
    "output1 = [[1]+make_output(phon, n=1) for phon in data.p] # vowel\n",
    "output2 = [[1]+make_output(phon, n=2) for phon in data.p] # coda\n",
    "output3 = [[1]+make_output(phon, n=3) for phon in data.p] # tone\n",
    "output4 = [[0]*(len(seq)-1)+[1] for seq in output0] # end condition\n",
    "\n",
    "### padding ###\n",
    "output0 = pad_sequences(output0, padding='post', maxlen=20, value=0)\n",
    "output1 = pad_sequences(output1, padding='post', maxlen=20, value=0)\n",
    "output2 = pad_sequences(output2, padding='post', maxlen=20, value=0)\n",
    "output3 = pad_sequences(output3, padding='post', maxlen=20, value=0)\n",
    "output4 = pad_sequences(output4, padding='post', maxlen=20, value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 33 29 27 31 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[ 1  2  2 12  2  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[ 1 16 16 16  3  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[1 3 5 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "i = 100\n",
    "\n",
    "print(output0[i])\n",
    "print(output1[i])\n",
    "print(output2[i])\n",
    "print(output3[i])\n",
    "print(output4[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "### one-hot vector ###\n",
    "output0 = to_categorical(output0, num_classes=len(onset2id)) # onset\n",
    "output1 = to_categorical(output1, num_classes=len(vowel2id)) # vowel\n",
    "output2 = to_categorical(output2, num_classes=len(coda2id)) # coda\n",
    "output3 = to_categorical(output3, num_classes=7) # tone 1-5, padding 0, start -1\n",
    "output4 = to_categorical(output4, num_classes=2)\n",
    "\n",
    "i = 100\n",
    "print(output3[i])\n",
    "print(output4[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31483, 55)\n",
      "(31483, 20, 40)\n",
      "(31483, 20, 23)\n",
      "(31483, 20, 17)\n",
      "(31483, 20, 7)\n",
      "(31483, 20, 2)\n"
     ]
    }
   ],
   "source": [
    "print(input_seq.shape)\n",
    "print(output0.shape) # onset\n",
    "print(output1.shape) # vowel\n",
    "print(output2.shape) # coda\n",
    "print(output3.shape) # tone\n",
    "print(output4.shape) # end condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25186, 55) (6297, 55)\n",
      "(25186, 20, 40) (6297, 20, 40)\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x = train_test_split(input_seq, test_size=0.2, random_state=1)\n",
    "train_y0, test_y0 = train_test_split(output0, test_size=0.2, random_state=1)\n",
    "train_y1, test_y1 = train_test_split(output1, test_size=0.2, random_state=1)\n",
    "train_y2, test_y2 = train_test_split(output2, test_size=0.2, random_state=1)\n",
    "train_y3, test_y3 = train_test_split(output3, test_size=0.2, random_state=1)\n",
    "train_y4, test_y4 = train_test_split(output4, test_size=0.2, random_state=1)\n",
    "\n",
    "print(train_x.shape, test_x.shape)\n",
    "print(train_y0.shape, test_y0.shape)\n",
    "\n",
    "input_len = train_x.shape[1]\n",
    "output_len = train_y0.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300\n",
    "lstm_dim = 200\n",
    "lstm_dropout = 0.2\n",
    "\n",
    "### ENCODER ###\n",
    "input_enc= layers.Input(batch_size=None, shape=(input_len,), name='Enc_Input')\n",
    "emb_enc = layers.Embedding(input_dim = data.input_vocab, # num of vocab = num of ID dict\n",
    "                                    input_length = data.input_maxlen,\n",
    "                                    output_dim = emb_dim,\n",
    "                                    mask_zero = True,\n",
    "                                    name = 'Enc_Embedding')\n",
    "lstm_enc = layers.Bidirectional(layers.LSTM(lstm_dim,\n",
    "                                        return_sequences=True, # use only final output when w/o attention -> return (batch, lstm_dim)\n",
    "                                        return_state=True,\n",
    "                                        recurrent_dropout=lstm_dropout,\n",
    "                                        name='Enc_LSTM'), name='BiDir') # return: output, h1, c1, h2, c2\n",
    "concat_h = layers.Concatenate(name='Concat_h') # concatnate bidirectional output of state h \n",
    "concat_c = layers.Concatenate(name='Concat_c') # concatnate bidirectional output of state c\n",
    "\n",
    "### DECODER ###\n",
    "\n",
    "class Attention:\n",
    "    def __init__(self, unit=256, emb_dim=256):\n",
    "        self.dot = layers.Dot(axes=[2,2], name='Attn_Dot') # (batch, dec_seq_length, \"lstm_dim\") * (batch, enc_seq_length, \"lstm_dim\") -> weight (batch, dec_seq_length, enc_seq_length) \n",
    "        #self.sqrt = layers.Lambda(lambda x: x/emb_dim**0.5, name='Sqrt') # scaling by square root : */√dim\n",
    "        self.softmax = layers.Activation(activation='softmax', name='Attn_Softmax')\n",
    "        self.context = layers.Dot(axes=[2,1], name='Attn_Context') # weight (batch, dec_seq_length, \"enc_seq_length\") * (batch, \"enc_seq_length\", dim) -> (batch, dec_seq_length, dim)\n",
    "        self.concat = layers.Concatenate(name='Attn_Concat') # concate context and decoder output\n",
    "        self.hidden = layers.Dense(unit, activation='tanh', name='Attn_hidden')\n",
    "\n",
    "    def __call__(self, enc_output, dec_output):\n",
    "        attention_dot = self.dot([dec_output, enc_output])\n",
    "        #attention_weight = self.softmax(self.sqrt(attention_dot))\n",
    "        attention_weight = self.softmax(attention_dot)\n",
    "        context_vector = self.context([attention_weight, enc_output])\n",
    "        concat_vector = self.concat([context_vector, dec_output])\n",
    "        return self.hidden(concat_vector)\n",
    "\n",
    "\n",
    "input_dec0 = layers.Input(batch_size=None, shape=(output_len,), name='Dec_Input0')\n",
    "input_dec1 = layers.Input(batch_size=None, shape=(output_len,), name='Dec_Input1')\n",
    "input_dec2 = layers.Input(batch_size=None, shape=(output_len,), name='Dec_Input2')\n",
    "input_dec3 = layers.Input(batch_size=None, shape=(output_len,), name='Dec_Input3')\n",
    "input_dec4 = layers.Input(batch_size=None, shape=(output_len,), name='Dec_Input4')\n",
    "input_h = layers.Input(batch_size=None, shape=(lstm_dim*2,), name='Dec_Input_h') # for predict\n",
    "input_c = layers.Input(batch_size=None, shape=(lstm_dim*2,), name='Dec_Input_c') # for predict\n",
    "emb_dec0 = layers.Embedding(input_dim=len(onset2id), input_length = output_len, output_dim=100, mask_zero = True, name = 'Dec_Emb0')\n",
    "emb_dec1 = layers.Embedding(input_dim=len(onset2id), input_length = output_len, output_dim=100, mask_zero = True, name = 'Dec_Emb1')\n",
    "emb_dec2 = layers.Embedding(input_dim=len(onset2id), input_length = output_len, output_dim=100, mask_zero = True, name = 'Dec_Emb2')\n",
    "emb_dec3 = layers.Embedding(input_dim=len(onset2id), input_length = output_len, output_dim=100, mask_zero = True, name = 'Dec_Emb3')\n",
    "emb_dec4 = layers.Embedding(input_dim=len(onset2id), input_length = output_len, output_dim=100, mask_zero = True, name = 'Dec_Emb4')\n",
    "concat_emb = layers.Concatenate(name='Concat_c') # concatnate bidirectional output of state c\n",
    "lstm_dec = layers.LSTM(lstm_dim, # twice as lstm dim of encoder\n",
    "                        return_sequences=True,\n",
    "                        return_state=True,\n",
    "                        recurrent_dropout=lstm_dropout,\n",
    "                        name='Dec_LSTM')\n",
    "dense = layers.Dense(data.output_vocab, activation='softmax', name='Dec_Hidden')\n",
    "        self.attention = Attention(unit=attention_dim, emb_dim=emb_dim)\n",
    "        self.enc_output = layers.Input(batch_size=None, shape=(lstm_dim), name='Enc_Output')\n",
    "        ### for inference - receive previous states\n",
    "        self.input_h = layers.Input(shape=lstm_dim, name='Dec_Input_h')\n",
    "        self.input_c = layers.Input(shape=lstm_dim, name='Dec_Input_c')\n",
    "\n",
    "class AttentionDecoder(Decoder):\n",
    "    def __init__(self, data:Data, emb_dim=256, lstm_dim=256, lstm_dropout=0.2, attention_dim=256):\n",
    "        super().__init__(data=data, emb_dim=emb_dim, lstm_dim=lstm_dim, lstm_dropout=lstm_dropout)\n",
    "        \n",
    "    \n",
    "    def __call__(self, state_h, state_c, enc_output=None):\n",
    "        x = self.input\n",
    "        x = self.embedding(x)\n",
    "        dec_output, h, c = self.lstm(x, initial_state=[state_h, state_c])\n",
    "        x = self.attention(enc_output, dec_output)\n",
    "        x = self.dense(x)\n",
    "        return x, h, c\n",
    "\n",
    "    def build(self):\n",
    "        output, h, c = self(state_h=self.input_h, state_c=self.input_c, enc_output=self.enc_output)\n",
    "        model = keras.models.Model(inputs=[self.input, self.enc_output, self.input_h, self.input_c], outputs=[output, h, c])\n",
    "        return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
